# ğŸ§  Cross-Modal GPT: Building a Multimodal LLM with Cross-Attention

ğŸš€ A hands-on project that brings together **vision and language** using the **Cross-Modality Attention** architecture â€” the same mechanism behind models like LLaVA, Flamingo, and LLaMA 3.2!

---

## ğŸ¯ Goal

To deeply understand how **multimodal large language models (MLLMs)** work by implementing a simplified version from scratch â€” using:

- ğŸ–¼ï¸ Vision Transformer (ViT) to encode image inputs
- ğŸ§  Transformer decoder (GPT-style) to generate or understand text
- ğŸ”€ Cross-attention layers to fuse image and text modalities

---

## ğŸ—ï¸ Architecture

          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Image (ViT)â”‚
          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Cross-Attention â”‚â—„â”€â”€â”€â”€â”€â”
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                â”‚                â”‚
                â–¼                â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
      â”‚  Transformer Decoder â”‚   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                   â”‚             â”‚
                   â–¼             â”‚
         Output Tokens (Text) â—„â”€â”€â”˜
            (Next-token prediction)


- Cross-attention is injected between decoder layers so that text can **attend to image embeddings**.
- The training is done via **next-token prediction**, just like standard GPT models.

---

## ğŸ§ª Minimal Training Setup

We use the **MNIST dataset** as a starting point for image-text alignment:

- Input: Image of a handwritten digit (e.g., \`7\`)
- Output: \`"This is the number seven."\`

This lets us train a character-level language model that *sees* and *describes* images â€” the simplest form of grounded image-text understanding.

---

## ğŸ“ Project Structure

\`\`\`
cross_modal_llm/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ vision_encoder.py           # ViT + projector
â”‚   â””â”€â”€ multimodal_decoder.py       # GPT-style decoder with cross-attn
â”œâ”€â”€ data/
â”‚   â””â”€â”€ mnist_dataset.py            # Image + generated text samples
â”œâ”€â”€ train.py                        # Main training loop
â””â”€â”€ config.py                       # Model and training configuration
\`\`\`

---

## ğŸ› ï¸ Tech Stack

- \`PyTorch\` for model training
- \`timm\` for pretrained ViT image encoder
- \`transformers\` (HuggingFace) for tokenizer
- Dataset: \`MNIST\` (plans to expand to EMNIST, synthetic OCR tasks, and VQA)

---

## ğŸŒ Roadmap

- [x] Build cross-attention fusion architecture
- [x] Train on MNIST + text descriptions
- [ ] Integrate real-world datasets (EMNIST, OCR, VQA-style)
- [ ] Switch from custom decoder to GPT-2 (\`transformers\`) with injected cross-attn
- [ ] Evaluate on OCR/captioning benchmarks
- [ ] Add OpenCLIP and better vision encoders
- [ ] Convert to instruction-following multimodal model

---

## ğŸ¤ Looking for Collaborators!

This project is an educational deep dive into the internals of multimodal LLMs. If youâ€™re:

- Passionate about LLMs and transformers
- Excited by cross-modality and multimodal learning
- Looking to contribute to open-source or research-backed projects

Drop a message or raise an issue! Letâ€™s build the future of vision-language AI â€” from the ground up. ğŸŒŸ

---

## ğŸ“š Inspiration

This project is inspired by:

- [Flamingo (DeepMind)](https://arxiv.org/abs/2204.14198)
- [LLaVA (UCSD & Microsoft)](https://github.com/haotian-liu/LLaVA)
- [Fuyu (Adept)](https://huggingface.co/adept/fuyu-8b)
- [NVLM (NVIDIA)](https://arxiv.org/abs/2403.02598)

---

## ğŸ§‘â€ğŸ’» Author

**Sushant Pargaonkar** â€” Data Scientist | AI Engineer  
_Always learning. Always building._

[GitHub](https://github.com/sushant-97) â€¢ [LinkedIn](https://www.linkedin.com/in/spargaonkar)

---

## ğŸ“„ License

MIT License â€” free to use, modify, and contribute.
